{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a81dba6",
   "metadata": {},
   "source": [
    "# Programación para el Análisis de Datos\n",
    "## Tarea - 2 Desarrollo de código para el tratamiento y manejo de información - Ejercicio 2\n",
    "**07 - **04 - 2024****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29055a78",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En la era actual, donde el volumen de datos crece exponencialmente, la ciencia de datos se posiciona como un campo crucial para la extracción de conocimiento y la toma de decisiones basadas en datos. Dentro de este dominio, la limpieza y el procesamiento de datos representan etapas fundamentales para asegurar la calidad y la fiabilidad de los análisis posteriores. Este trabajo se ha centrado en explorar técnicas avanzadas de manipulación de datos, particularmente en el tratamiento de valores faltantes y la manipulación de strings, utilizando la librería Pandas en Python. Estas operaciones son esenciales para transformar datos brutos en conjuntos de datos listos para el análisis, especialmente cuando se trata de información textual en diversos contextos de la ciencia de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d15ac",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Limpieza y procesamiento de datos\n",
    "\n",
    "### 1. Parte \n",
    "\n",
    "## Revisión teórica de Limpieza y procesamiento de datos\n",
    "\n",
    "La limpieza y el procesamiento de datos son pasos cruciales en cualquier proyecto de análisis de datos o aprendizaje automático. Estos procesos implican la transformación de los datos crudos en un formato que es más adecuado para el análisis, lo que puede incluir la corrección de errores, la eliminación de datos innecesarios y la preparación de los datos para su análisis. A continuación, se presenta una revisión teórica de estos conceptos clave:\n",
    "\n",
    "### 1. **Limpieza de Datos**\n",
    "\n",
    "La limpieza de datos se refiere al proceso de detectar y corregir (o eliminar) registros corruptos o inexactos de un conjunto de datos. Incluye varias tareas como:\n",
    "\n",
    "- **Detección y manejo de valores faltantes**: Esto puede implicar la eliminación de registros con valores faltantes, la imputación de estos valores utilizando métodos estadísticos, o la asignación de un valor predeterminado.\n",
    "  \n",
    "- **Identificación y corrección de errores**: Los errores pueden surgir de diversas fuentes, como errores de entrada de datos o problemas en la recopilación de datos. La corrección de estos errores es esencial para mantener la integridad de los datos.\n",
    "  \n",
    "- **Eliminación de duplicados**: Los registros duplicados pueden distorsionar el análisis, por lo que es importante identificarlos y eliminarlos.\n",
    "\n",
    "- **Normalización de datos**: Este proceso involucra la transformación de los datos a una escala común, lo que es particularmente útil en análisis que involucran múltiples variables de diferentes escalas.\n",
    "\n",
    "### 2. **Procesamiento de Datos**\n",
    "\n",
    "Una vez que los datos están limpios, el procesamiento de datos implica transformarlos en un formato que es más adecuado para el análisis. Esto puede incluir:\n",
    "\n",
    "- **Transformación de datos**: Cambiar los datos de su formato original a un formato necesario para el análisis. Esto puede incluir la creación de variables derivadas, la categorización de variables continuas, o la transformación logarítmica de variables para normalizar su distribución.\n",
    "\n",
    "- **Codificación de variables categóricas**: Muchos algoritmos de aprendizaje automático requieren que los datos de entrada sean numéricos. La codificación de variables categóricas (por ejemplo, mediante la codificación one-hot) convierte estas variables en un formato que los algoritmos pueden procesar.\n",
    "\n",
    "- **División de datos**: En muchos casos, especialmente en el aprendizaje automático, los datos se dividen en conjuntos de entrenamiento, validación y prueba. Esta división ayuda a evaluar el rendimiento del modelo de manera más efectiva.\n",
    "\n",
    "- **Estandarización o normalización de datos**: La estandarización implica reescalar las características para que tengan una media de 0 y una desviación estándar de 1. La normalización, por otro lado, reescala las características para que estén en un rango entre 0 y 1. Ambos procesos pueden ser cruciales dependiendo del modelo o algoritmo utilizado.\n",
    "\n",
    "### 3. **Herramientas y Técnicas**\n",
    "\n",
    "Existen numerosas herramientas y técnicas para facilitar la limpieza y el procesamiento de datos, incluidas bibliotecas de programación como Pandas y NumPy en Python, que ofrecen una amplia gama de funciones para manipular y preparar datos. Además, software especializado como Tableau o Power BI pueden proporcionar capacidades avanzadas de limpieza de datos con interfaces más intuitivas.\n",
    "\n",
    "### 4. **Importancia de la Limpieza y Procesamiento de Datos**\n",
    "\n",
    "La calidad de los datos es fundamental para obtener resultados confiables en cualquier análisis. Datos mal limpiados o incorrectamente procesados pueden llevar a conclusiones erróneas, lo que subraya la importancia de estos procesos en el flujo de trabajo de análisis de datos.\n",
    "\n",
    "En resumen, la limpieza y el procesamiento de datos son pasos fundamentales que preparan el conjunto de datos para el análisis, asegurando que los insights generados sean válidos y confiables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be446d34",
   "metadata": {},
   "source": [
    "### 2. Parte\n",
    "\n",
    "### Descripción del ejercicio:\n",
    "\n",
    "ParPara este ejercicio, cargue de nuevo los datos del Ejercicio 1, pero genere un DataFrame llamado df, solamente con las columnas:\n",
    "\n",
    "• type\n",
    "\n",
    "• mass_kg\n",
    "\n",
    "• year_built\n",
    "\n",
    "• home_port\n",
    "\n",
    "• name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b0ea693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    type   mass_kg  year_built            home_port                     name\n",
      "0    Tug  266712.0      1976.0  Port of Los Angeles        American Champion\n",
      "1  Cargo       NaN         NaN  Port of Los Angeles        American Islander\n",
      "2  Cargo       NaN         NaN  Port of Los Angeles          American Spirit\n",
      "3  Barge       NaN      2021.0       Port Canaveral  A Shortfall of Gravitas\n",
      "4    Tug  202302.0      1974.0  Port of Los Angeles       Betty R Gambarella\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL de la API de SpaceX para obtener datos de las naves\n",
    "url = 'https://api.spacexdata.com/v4/ships'\n",
    "\n",
    "# Realizar la petición GET a la API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar que la petición fue exitosa\n",
    "if response.status_code == 200:\n",
    "    # Convertir la respuesta en formato JSON a un objeto Python\n",
    "    data = response.json()\n",
    "    \n",
    "    # Generar un DataFrame con las columnas específicas\n",
    "    df = pd.DataFrame(data)[['type', 'mass_kg', 'year_built', 'home_port', 'name']]\n",
    "\n",
    "    # Mostrar las primeras filas del DataFrame para verificar\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Error en la petición a la API:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adce0a",
   "metadata": {},
   "source": [
    "Siguiendo las funciones mostradas en las referencias bibliográficas, desarrolle las siguientes tareas en el DataFrame generado\n",
    "anteriormente:\n",
    "\n",
    "### 3. Parte\n",
    "\n",
    "### Revisando datos faltantes:\n",
    "\n",
    "• Determine cuantos datos faltantes hay en la variable mass_kg del mediante value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f2cbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos faltantes en 'mass_kg': 8\n"
     ]
    }
   ],
   "source": [
    "# Contar los valores nulos en la columna 'mass_kg'\n",
    "missing_mass_kg = df['mass_kg'].isna().sum()\n",
    "\n",
    "# Mostrar la cantidad de valores nulos\n",
    "print(\"Cantidad de datos faltantes en 'mass_kg':\", missing_mass_kg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a198e3",
   "metadata": {},
   "source": [
    "### 4. Parte \n",
    "\n",
    "Crea un DataFrame df1 removiendo las filas que tienen algunos datos faltantes. Por otro lado crea un DataFrame df2 removiendo\n",
    "las filas que tienen todos los datos faltantes. Compáralos y comenta los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6c8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df1 eliminando filas con al menos un dato faltante\n",
    "df1 = df.dropna()\n",
    "\n",
    "# Crear df2 eliminando filas donde todos los datos son faltantes\n",
    "df2 = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1ce497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad original de filas en 'df': 29\n",
      "Cantidad de filas en 'df1' (sin ningún dato faltante): 21\n",
      "Cantidad de filas en 'df2' (sin filas completamente vacías): 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad original de filas en 'df':\", len(df))\n",
    "print(\"Cantidad de filas en 'df1' (sin ningún dato faltante):\", len(df1))\n",
    "print(\"Cantidad de filas en 'df2' (sin filas completamente vacías):\", len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da0e24",
   "metadata": {},
   "source": [
    "### Comparación y comentarios de los resultados.\n",
    "\n",
    "La comparación entre `df`, `df1` y `df2` revela lo siguiente:\n",
    "\n",
    "- **`df` (DataFrame Original)**: Contiene 29 filas en total. Este es el punto de partida antes de aplicar cualquier filtrado por datos faltantes.\n",
    "\n",
    "- **`df1` (Sin ningún dato faltante)**: Al eliminar filas que contienen al menos un valor faltante, el total de filas se reduce a 21. Esto indica que 8 filas en el DataFrame original tenían al menos un valor faltante en alguna de sus columnas. Al utilizar este enfoque, se garantiza que todas las filas restantes en `df1` tienen valores completos en todas las columnas, lo que puede ser crítico para ciertos análisis que requieren datos completos. Sin embargo, este método también puede resultar en la pérdida de datos significativos, especialmente si los valores faltantes están concentrados en una o pocas columnas que quizás no sean críticas para el análisis.\n",
    "\n",
    "- **`df2` (Sin filas completamente vacías)**: Al eliminar solo las filas donde todos los datos son faltantes, se observa que el número de filas se mantiene igual que en el DataFrame original (`df`), con un total de 29 filas. Esto indica que no había filas en el conjunto de datos original donde todos los valores fueran `NaN`. Por lo tanto, `df2` mantiene todas las filas originales, incluyendo aquellas con algunos valores faltantes.\n",
    "\n",
    "### Comentarios sobre los resultados:\n",
    "\n",
    "- La diferencia en el número de filas entre `df1` y `df` muestra la cantidad de filas que tenían al menos un valor faltante. En contextos donde la integridad de los datos en todas las columnas es crucial, utilizar un enfoque como el de `df1` es preferible, aunque conlleva el riesgo de perder información potencialmente valiosa.\n",
    "\n",
    "- El hecho de que `df2` tenga el mismo número de filas que `df` indica que no había filas en el conjunto de datos original que estuvieran completamente vacías. Esto sugiere que todos los registros tenían al menos algún dato en una o más columnas. `df2` es útil en análisis donde se pueden tolerar algunos datos faltantes y se prefiere retener la mayor cantidad posible de registros.\n",
    "\n",
    "- En la práctica, la elección entre `df1` y `df2` dependerá del análisis específico a realizar y de la importancia de tener conjuntos de datos completos frente a la necesidad de retener tantos registros como sea posible. Cada enfoque tiene sus ventajas y limitaciones, y la decisión debe basarse en el contexto y los requisitos específicos del análisis en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dd40a",
   "metadata": {},
   "source": [
    "### 5. Parte\n",
    "\n",
    "En el DataFrame df completa los datos faltantes por los siguientes tres métodos (muestra y comenta los resultados):\n",
    "\n",
    "- Completar datos faltantes con el número 0.\n",
    "- Completar por método de propagación hacia adelante.\n",
    "- Completar los valores faltantes con el promedio de la columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dd496",
   "metadata": {},
   "source": [
    "#### 1. Completar datos faltantes con el número 0\n",
    "\n",
    "Este método consiste en reemplazar todos los valores faltantes (NaN) por el número 0. Es útil cuando tiene sentido dentro del contexto de los datos que los valores faltantes puedan considerarse como \"cero\" en términos prácticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea22d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled_zero = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0bc43",
   "metadata": {},
   "source": [
    "#### 2. Completar por método de propagación hacia adelante\n",
    "\n",
    "La propagación hacia adelante (ffill) toma el último valor válido (no nulo) y lo utiliza para reemplazar los valores faltantes hasta que encuentre el próximo valor válido. Es especialmente útil en series temporales o cuando el orden de los datos importa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57821877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled_ffill = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648cfa0",
   "metadata": {},
   "source": [
    "#### 3. Completar los valores faltantes con el promedio de la columna\n",
    "\n",
    "Este método implica calcular el promedio (media) de los valores no nulos en una columna y utilizar este promedio para reemplazar los valores faltantes. Es una buena opción cuando los datos faltantes pueden considerarse representativos del promedio de los datos existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3a787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    type        mass_kg   year_built            home_port  \\\n",
      "0    Tug  266712.000000  1976.000000  Port of Los Angeles   \n",
      "1  Cargo  916838.095238  1998.461538  Port of Los Angeles   \n",
      "2  Cargo  916838.095238  1998.461538  Port of Los Angeles   \n",
      "3  Barge  916838.095238  2021.000000       Port Canaveral   \n",
      "4    Tug  202302.000000  1974.000000  Port of Los Angeles   \n",
      "\n",
      "                      name  \n",
      "0        American Champion  \n",
      "1        American Islander  \n",
      "2          American Spirit  \n",
      "3  A Shortfall of Gravitas  \n",
      "4       Betty R Gambarella  \n"
     ]
    }
   ],
   "source": [
    "# Calcular el promedio solo para las columnas numéricas\n",
    "mean_values = df.mean(numeric_only=True)\n",
    "\n",
    "# Utilizar estos valores promedio para rellenar los datos faltantes en las columnas numéricas\n",
    "df_filled_mean = df.fillna(mean_values)\n",
    "\n",
    "# Mostrar las primeras filas para verificar los cambios\n",
    "print(df_filled_mean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2cf88",
   "metadata": {},
   "source": [
    "### 6. Parte\n",
    "#### Operaciones de strings:\n",
    "\n",
    "Realice las siguientes operaciones en el DataFrame df:\n",
    "\n",
    "1. De la columna home_port extraiga la primera palabra de cadadato y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b7a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Port\n",
      "1    Port\n",
      "2    Port\n",
      "3    Port\n",
      "4    Port\n",
      "Name: first_word_home_port, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['first_word_home_port'] = df['home_port'].str.split().str[0]\n",
    "print(df['first_word_home_port'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c1bfe",
   "metadata": {},
   "source": [
    "2. Determine cuántos datos comienzan con la palabra Port y cuántos datos comienzan con una palabra diferente, ¿Cuáles son las palabras diferentes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10967938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos que comienzan con 'Port': 28\n",
      "Datos que comienzan con una palabra diferente: 1\n",
      "Palabras diferentes: ['Fort']\n"
     ]
    }
   ],
   "source": [
    "starts_with_port = df['home_port'].str.startswith('Port').sum()\n",
    "starts_with_other = (~df['home_port'].str.startswith('Port')).sum()\n",
    "\n",
    "# Palabras diferentes que no son 'Port'\n",
    "different_words = df.loc[~df['home_port'].str.startswith('Port'), 'home_port'].str.split().str[0].unique()\n",
    "\n",
    "print(f\"Datos que comienzan con 'Port': {starts_with_port}\")\n",
    "print(f\"Datos que comienzan con una palabra diferente: {starts_with_other}\")\n",
    "print(f\"Palabras diferentes: {different_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d8bbd",
   "metadata": {},
   "source": [
    "3. Filtre el DataFrame con los datos que en la columna namecontengan en el string la palabra “GO” y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482b0037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name\n",
      "6   GO Ms Chief\n",
      "7    GO Ms Tree\n",
      "9    GO Pursuit\n",
      "10     GO Quest\n"
     ]
    }
   ],
   "source": [
    "df_with_go = df[df['name'].str.contains('GO')]\n",
    "print(df_with_go[['name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312b34",
   "metadata": {},
   "source": [
    "4. Convierta todos los strings de la columna name en palabras con todas sus letras mayúsculas y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9009670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          AMERICAN CHAMPION\n",
      "1          AMERICAN ISLANDER\n",
      "2            AMERICAN SPIRIT\n",
      "3    A SHORTFALL OF GRAVITAS\n",
      "4         BETTY R GAMBARELLA\n",
      "Name: name_uppercase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['name_uppercase'] = df['name'].str.upper()\n",
    "print(df['name_uppercase'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061438d",
   "metadata": {},
   "source": [
    "5. Convierta todos los strings de la columna type en palabras con todas sus letras minúsculas y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a8a2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      tug\n",
      "1    cargo\n",
      "2    cargo\n",
      "3    barge\n",
      "4      tug\n",
      "Name: type_lowercase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['type_lowercase'] = df['type'].str.lower()\n",
    "print(df['type_lowercase'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0f783",
   "metadata": {},
   "source": [
    "6. Divida todos los strings de la columna home_port por el espacio en blanco y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f70f6ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Port, of, Los, Angeles]\n",
      "1    [Port, of, Los, Angeles]\n",
      "2    [Port, of, Los, Angeles]\n",
      "3           [Port, Canaveral]\n",
      "4    [Port, of, Los, Angeles]\n",
      "Name: home_port_split, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['home_port_split'] = df['home_port'].str.split(' ')\n",
    "print(df['home_port_split'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90af2d7",
   "metadata": {},
   "source": [
    "7. Reemplace todas las palabras ‘Port’ que se encuentran en la columna home_port por la abreviación ‘P.’ y muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e97b767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    P. of Los Angeles\n",
      "1    P. of Los Angeles\n",
      "2    P. of Los Angeles\n",
      "3         P. Canaveral\n",
      "4    P. of Los Angeles\n",
      "Name: home_port_replaced, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['home_port_replaced'] = df['home_port'].str.replace('Port', 'P.')\n",
    "print(df['home_port_replaced'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d85161",
   "metadata": {},
   "source": [
    "## Comentarios\n",
    "\n",
    "- La extracción de la primera palabra de home_port puede ser útil para análisis o categorización basada en ubicaciones más generales.\n",
    "- Determinar el comienzo de los datos con \"Port\" y encontrar alternativas nos ayuda a entender mejor la distribución geográfica de los datos y posiblemente normalizar la nomenclatura de los puertos.\n",
    "- Filtrar datos por contenido específico en los nombres (como \"GO\") permite identificar rápidamente subconjuntos de datos de interés, lo cual es crucial en el análisis de datos y la toma de decisiones.\n",
    "- La conversión de texto a mayúsculas o minúsculas puede ser fundamental para la estandarización de los datos de texto, lo que facilita su comparación y filtrado.\n",
    "- Dividir strings por espacios o reemplazar partes de strings son herramientas poderosas para la manipulación y limpieza de datos textuales, permitiendo reformatear la información de manera que sea más útil para análisis específicos.\n",
    "\n",
    "Estas herramientas de manipulación de strings en Pandas son extremadamente útiles en la limpieza de datos, el preprocesamiento, y la extracción de características, especialmente en campos que involucran el manejo de grandes volúmenes de datos textuales o la necesidad de normalizar y estandarizar datos de texto para análisis posteriores. En el campo de la ciencia de datos, las técnicas de manipulación de strings en Pandas son clave para limpiar y preparar datos textuales, esenciales en procesos de análisis y modelado, especialmente en NLP. Permiten normalizar y estructurar datos crudos, facilitando la extracción de insights y mejorando el rendimiento de modelos de machine learning. Estas habilidades son cruciales en áreas como análisis de sentimientos y clasificación de texto, convirtiéndose en herramientas indispensables para los científicos de datos que buscan profundizar en el vasto mundo de los datos no estructurados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e591f7",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "**1.** La manipulación eficiente de datos, incluyendo el manejo de valores faltantes y la operación avanzada con strings, es un pilar fundamental en la ciencia de datos. Estas técnicas no solo mejoran la calidad de los conjuntos de datos, sino que también permiten una exploración más profunda y la generación de insights más ricos. En un campo donde los datos no estructurados son abundantes, especialmente en forma de texto, dominar estas habilidades se convierte en un requisito indispensable para cualquier científico de datos. En última instancia, el manejo competente de los datos no solo facilita el análisis y la modelización, sino que también impulsa la innovación y la creación de valor a partir de grandes volúmenes de información.\n",
    "\n",
    "**1.** Flexibilidad y Adaptabilidad en la Imputación de Datos: Este trabajo subraya la importancia de seleccionar el método de imputación más adecuado para cada situación específica. No existe una solución única para todos los casos, y la elección entre rellenar con ceros, usar la propagación hacia adelante o imputar con el promedio depende de la naturaleza de los datos y del contexto analítico. Esta flexibilidad es crucial para mantener la integridad de los datos y asegurar que las decisiones tomadas durante la fase de limpieza y preprocesamiento no introduzcan sesgos inadvertidos en el análisis posterior.\n",
    "\n",
    "**2.** Importancia de la Manipulación Avanzada de Strings en la Extracción de Características: La capacidad de realizar operaciones complejas con strings es fundamental para desbloquear el valor completo de los conjuntos de datos textuales, especialmente en la era del big data y el NLP. Las técnicas exploradas en este trabajo no solo facilitan la limpieza y normalización de los datos, sino que también abren caminos hacia la extracción de características más sofisticadas, lo que puede mejorar significativamente el rendimiento de los modelos de machine learning y proporcionar insights más profundos y matizados a partir de datos textuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74603aab",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas\n",
    "\n",
    "**1.** Samir Madhavan. (2015). Mastering Python for Data Science: Explore the World of Data Science Through Python and Learn How to Make Sense of Data. Packt Publishing. (pp. 11-18).\n",
    "\n",
    "**2.** Thakur, A. (2016). Python: Real-World Data Science. Packt Publishing. (pp. 511 – 538).\n",
    "\n",
    "**3.** ¿En qué consiste la limpieza de datos? - Explicación de la limpieza de datos - AWS. (n.d.). Amazon Web Services, Inc. https://aws.amazon.com/es/what-is/data-cleansing/\n",
    "\n",
    "**4.** Team, A. A. (2024, April 2). What is data processing? definition and stages. Astera. https://www.astera.com/es/knowledge-center/what-is-data-processing-definition-and-stages/#:~:text=El%20procesamiento%20de%20datos%20implica,legibles%2C%20como%20gr%C3%A1ficos%20o%20documentos.\n",
    "\n",
    "**5.** Torrenti, Á. (2024, April 5). Descubre la librerías Pandas y NumPy en Python. Imagina Formación. https://imaginaformacion.com/tutoriales/pandas-numpy-en-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29170ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
